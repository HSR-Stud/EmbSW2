\section{Algorithmen - x}

\subsection{Beschreibungsmethoden}
\begin{itemize}
	\item Aktivitätsdiagramm $\rightarrow$ gute Variante!
	\item Konkrete Programmiersprache $\rightarrow$ gute Variante!
	\item Pseudocode
	\item Struktogramme $\rightarrow$ veraltet
	\item Programmablaufpläne $\rightarrow$ veraltet
\end{itemize}

\subsection{Formale Eigenschaften}
\subsubsection{Korrektheit}
    \begin{itemize}
	    \item Die Korrektheit eines Algorithmus kann formal bewiesen werden. Dies ist jedoch häufig mit enormem Aufwand verbunden und wird in der Praxis nur selten durchgeführt $\rightarrow$ In der Praxis wird die Korrektheit mit Tests verifiziert.
	    \item Nur \textbf{Anwesenheit von Fehlern} kann bewiesen werden, nicht deren Abwesenheit [E.Dijkstra]
    \end{itemize}
\subsubsection{Finitheit}
    \begin{itemize}
	    \item Der Algorithmus muss eindeutig mit einer endlichen Anzahl Schritten beschrieben werden können.
    \end{itemize}
\subsubsection{Bestimmtheit}
    \begin{itemize}
	    \item Der Algorithmus erzielt bei gleichen Vorbedingungen immer dasselbe Resultat (bei Algorithmen, die mit Zufallskomponenten arbeiten, trifft das nicht zu).
    \end{itemize}
\subsubsection{Determinismus}
    \begin{itemize}
	    \item Zu jedem Zeitpunkt ist der nächste Schritt des Algorithmus eindeutig bestimmt.
    \end{itemize}
\subsubsection{Effizienz}
    \begin{itemize}
	    \item Laufzeiteffizienz
	    \item Speicherplatzeffizienz
	    \item Verschiedene Betrachtungsweisen:
	    \begin{itemize}
	        \item Best Case: Optimistischer Fall
	        \item Average Case: Durchschnittlicher Fall
	        \item Worst case (üblich): Pessimistischer Fall
	    \end{itemize}
    \end{itemize}
\subsubsection{Optionale formale Eigenschaften}
    \begin{itemize}
	    \item Einfachheit
	    \item Verständlichkeit
    \end{itemize}


\subsection{Komplexitätstheorie}
\subsubsection{Hauptziele}
    \begin{itemize}
        \item Berechngungskomplexität: Zeitkomplexität als Anzahl der Rechenoperationen sowie Speicherplatzkomplexität
        \item Spezifikation der Probleme und Methode zur Klassifikation in "'praktisch lösbare"' und "'praktisch unlösbare"' Probleme
        \item Vergleiche der Effizienz (Berechnungsstärke) verschiedener Algorithmen (deterministischer, nichtdeterministischer und zufallsgesteuerten)
    \end{itemize}
\subsubsection{Zu betrachtende Operationen}
    \begin{itemize}
        \item Zuweisungen (Bewegung von Datensätzen)
        \item Vergleiche
        \item Rechenoperationen
        \item Unterprogrammaufrufe
        \item Verzweigungen
    \end{itemize}

\subsubsection{Asymptotische Abschätzung}
Die asymptotische Abschätzung ist eine Worst-Case Analyse und kann in folgende Notationen aufgeteilt werden:\\

\textbf{$\mathcal{O}$-Notation (Häufigste Abschätzung)}\\
Laufzeitabschätzung nach oben:
\begin{equation}
	\mathcal{O}(g) = {f: X \rightarrow X \mid \exists c_1>0 \wedge \exists c_2>0 \wedge \exists n_0 \in X  \forall n \geq n_0 : f(n) \leq c_1\cdot g(n) + c_2}
\end{equation}
Finde eine Funktion g(n), deren Aufwandsfunktion $f(n) \leq c_1 \cdot g(n) + c_2$, dann ist der Aufwand O(g).\\
Beispiel: Gegeben sei die Aufwandfunktion $f(n)=3n^2 + 6n + 7$. Die Funktion f(n) ist ein Element der Menge $O(n^2)$, da z.B. $f(n) \leq 9n^2 + 7$ \ \ $\forall n \geq 0$.\\

\textbf{$\Omega$-Notation}\\
Laufzeitabschätzung nach unten: 
\begin{equation}
\Omega(g) = {f: X \rightarrow X \mid \exists c>0 \wedge \exists n_0 \in X  \forall n \geq n_0 : f(n) \geq c\cdot g(n)}
\end{equation}
Finde eine Funktion g(n), sodass die Aufwandfunktion $f(n) \geq c \cdot g(n)$, dann ist der Aufwand $\Omega(g)$.\\

\textbf{$\Theta$-Notation (Theta-Notation}\\
O- und $\Omega$ Notation vereint: 
\begin{equation}
\Theta(g)= O(g) \cap \Omega(g)
\end{equation}\\

\textbf{Bemerkungen zur asymptotischen Abschätzung}\\
In der Praxis wird bei der asymptotischen Abschätzung fast ausschliesslich die asymptotische Abschätzung nach
oben, d.h. O() verwendet. Die asymptotische Abschätzung ist ein gutes Mass für den schlechtesten Fall bei sehr
grossen n. Konstante Faktoren werden nicht betrachtet, obwohl sie alles andere als unerheblich sein können.\\
Es ist durchaus möglich, dass ein bestimmter Algorithmus ausschliesslich mit kleinen n arbeitet und damit nie in ein
für grosse n relevantes starkes Wachstum gerät. Es ist deshalb wichtig, dass der Anwender von asymptotischer Abschätzung
genau weiss, welche Schlüsse er daraus ziehen kann und vor allem auch welche nicht.\\

\subsubsection{Wachstumsfunktionen}
Die häufigsten und wichtigsten Wachstumsfunktionen sind:
\begin{itemize}
\item \textbf{Konstantes Wachstum: 1}\\
			Die Anweisungen eines Programms werden einmal oder höchstens mehrere Male durchlaufen, unabhängig von der Grösse des Problems. Die Laufzeit des Programms ist konstant. Dies ist der Idealfall des Laufzeitverhaltens.
\item \textbf{Logarithmisches Wachstum: $\log(n)$}\\
			Die Laufzeit des Algorithmus wird mit steigender Grösse n nur allmählich langsamer. Genauer: wenn n quadriert wird, wächst log n nur um einen konstanten Betrag. Beim Logarithmus ist die Basis nicht relevant, da sie als konstanter Faktor herausgezogen werden kann.
\item \textbf{Lineares Wachstum: n}\\
			Die Laufzeit des Algorithmus steigt mit steigender Grösse n linear an.
\item \textbf{n-log-n Wachstum: $n\cdot \log(n)$}\\
			Dieses Wachstum tritt häufig bei Algorithmen auf, die ein Problem lösen, indem sie es in kleinere Teilprobleme aufteilen, diese unabhängig voneinander lösen und dann die Lösungen kombinieren. Das n-log n-Wachstum ist nicht wesentlich grösser als lineares Wachstum. 
\item \textbf{Polynomiales Wachstum: $n^c$}\\
			Algorithmen mit polynomialem Wachstum (quadratisch, kubisch, ...) sind eigentlich nur dann vertretbar, wenn einem nichts anderes übrigbleibt. Es ist das Wachstum, das in der Praxis noch als äusserstes generell akzeptiert wird (Probleme der Komplexitätsklasse P). Typischerweise entsteht z.B. dann quadratisches Wachstum, wenn paarweise alle Kombinationen von Datenelementen verarbeitet werden. Dies geschieht häufig mit zwei ineinander verschachtelten Schleifen beispielsweise in Matrixoperationen.  
\item \textbf{Exponentielles Wachstum: $b^n$}\\
			Algorithmen mit exponentiellem Wachstum sind nur für sehr seltene Probleme in der Praxis verwendbar, da die Laufzeit mit steigendem n förmlich explodiert. Aktuell sind ein paar Tausend relevante Probleme der Komplexitätsklasse NP identifiziert, die auf heute vorhandenen Rechnern nur mit exponentieller Komplexität gelöst werden können, z.B. das Travelling Salesman Problem (TSP). 
\end{itemize}

\includegraphics[width=0.5\textwidth]{images/Algorithmen/Wachstumsfunktionen.png}

\subsubsection{Komplexitätsklassen}
Probleme mit ähnlicher Komplexität werden in sogenannten Komplexitätsklassen zusammengefasst. Klassen werden als obere Schranke für den Bedarf einer bestimmten Ressource definiert, in Frage kommen dabei in der Praxis der Zeitbedarf (Zeitkomplexität) und der Speicherbedarf (Platzkomplexität). Auch hier handelt es sich um eine Worst-case-Betrachtung.\\

DTM (Deterministische Turingmaschine): Für jeden Zustand bei gegebenem Input ist der nächste Zustand eindeutig definiert. Ist praktisch realisierbar.\\

NTM (Nicht deterministische Turingmaschine): Für einen Zustand bei gegebenem Input ist der nächste Zustand \textbf{nicht} eindeutig definiert. Heute noch nicht realisierbar, dient jedoch als Algorithmusausführungsmodell.\\

Folgende Klassen sind definiert:
\begin{itemize}
\item NL:       Von NTM auf logarithmischen Platz lösbar 
\item P:        Von DTM in polynomialer Zeit lösbar, $O(n^c)$
\item NP:       Von NTM in polynomialer Zeit lösbar, $O(n^c)$
\item PSPACE:   Von DTM auf polynomialem Platz lösbar
\item EXPTIME:  Von DTM in exponentieller Zeit lösbar, $O(2^n)$
\item EXPSPACE: Von DTM auf exponentiellem Platz lösbar
\end{itemize}

Probleme der Klasse P lassen sich in vernünftiger Zeit lösen. \\
Viele Probleme der Klasse NP lassen sich vermutlich nicht effizient lösen.\\
Bestätigung oder Widerlegung dieses P-NP-Problems ist wohl das wichtigste offene Problem der Informatik.\\
Bekanntes NP-Problem ist das Travelling Salesman Problem.\\


\subsection{Suchalgorithmen}
\subsubsection{Listensuche}
Finde Wert in einer Liste.\\

\textbf{Listendefinition}
\lstinputlisting[language=C++]{code/Listendefinition.cpp}
\ \newline
\textbf{Vereinfachte Listendefinition}
\lstinputlisting[language=C++]{code/VereinfachteListendefinition.cpp}
\ \newline
\textbf{Variante 1: Lineare Suche}\\
Die einfachste Suchmethode ist, von vorne nach hinten Element um Element zu überprüfen bis das gesuchte Element gefunden oder das Listenende erreicht ist. Wenn nicht bekannt ist, ob die Liste irgendwie sortiert ist, ist das die einzige mögliche Suchmethode. \\
Komplexität: O(n)
\lstinputlisting[language=C++]{code/LineareSuche.cpp}
\ \newline
\textbf{Variante 2: Lineare Suche mit Sentinel}\\
Die einzige Methode, den obigen Algorithmus zu beschleunigen, besteht in der Vereinfachung der
Schleifenbedingung. Bei der linearen Suche mit Sentinel (Wächter) wird einfach am Ende der Liste ein Hilfselement eingefügt, das auf den Wert des gesuchten Elementes gesetzt wird. Der Algorithmus terminiert so auch ohne die Abfrage der Listenlänge, allerdings wird ein Platz in der Liste für den Sentinel geopfert.\\
Die Schleife wird etwas schneller durchlaufen, aber die Komplexität ist noch immer O(n).
\lstinputlisting[language=C++]{code/LineareSucheMitSentinel.cpp}
\ \newline
\textbf{Variante 3: Binäre Suche}\\
Diese Suche funktioniert nur, wenn die Liste der Reihe nach sortiert ist.\\
Man geht von der Mitte der Liste aus, ist der gegebene Wert grösser als die Mitte sucht man in der oberen Hälfte auf dieselbe Art weiter, ansonsten in der unteren Hälfte.\\
Liegt der Wert am Rande ist die Suche auf diese Art erfolglos.\\
Komplexität: O(log(n))\\
\lstinputlisting[language=C++]{code/BinaereSuche.cpp}
Beispiel "'Suche 53"':\\
\includegraphics[width=0.4\textwidth]{images/Algorithmen/BinaereSuche.png}

\subsubsection{Mustersuche}
Bei der Mustersuche (string search) geht es darum, ein ganzes Stringmuster (Pattern) in einem vorgegebenen Text zu suchen. Wenn nur ein einzelnes Zeichen gesucht werden müsste, wäre dies identisch mit der Listensuche. Bei der Suche eines ganzen Patterns wird der Algorithmus aber bedeutend komplexer. Da die Mustersuche eine fundamentale Operation in jeder Textverarbeitung ist, besteht hier ein grosses Interesse, einen möglichst effizienten Algorithmus zu finden.\\

\textbf{Variante Naiver Algorithmus}\\
Der naive Algorithmus ist sehr einfach, es wird einfach Buchstabe um Buchstabe verglichen, bis eine Diskrepanz eintritt. Anschliessend wird das Pattern um eine Position weitergeschoben und wieder
Buchstabe um Buchstabe verglichen. Wenn die Diskrepanz schon bei einem der ersten Buchstaben auftritt, ist die Performance des Algorithmus OK. Wenn jedoch viele Buchstaben übereinstimmen bis endlich eine Diskrepanz auftritt, dann ist die Performance sehr schlecht. Im schlechtesten Fall ist der Algorithmus O(n·m), wobei n die Länge des Textes und m die Länge des Patterns ist.
\lstinputlisting[language=C++]{code/NaiverAlgorithmus.cpp}
Bei effizienteren Suchalgorithmen (z.B. KMP-Algorithmus oder BM-Algorithmus) wird bei einer Nichtübereinstimmung nicht nur um ein Zeichen sondern um eine möglichst hohe Anzahl Zeichen geschoben. Die Intelligenz liegt hier beim Herausfinden dieser Anzahl.\\


\subsection{Sortieralgorithmen}

\subsubsection{Interne vs. externe Sortierverfahren}
\begin{itemize}
    \item Bei den \textbf{internen Sortierverfahren} besteht auf sämtliche Daten ein wahlfreier Zugriff (random access). Die Daten liegen vollständig im Hauptspeicher vor. Die internen Sortierverfahren werden auch als Verfahren zum Sortieren von Feldern (Arrays ) bezeichnet.
    \item Bei den \textbf{externen Sortierverfahren} kann nur sequentiell auf die Daten zugegriffen werden. Die Daten sind in Files auf langsameren externen Massenspeichern wie z.B. Magnetbändern oder Harddisks gespeichert. Die externen Sortierverfahren werden auch als Verfahren zum Sortieren von Files bezeichnet.
\end{itemize}

\subsubsection{Forderungen an interne Sortierverfahren}
\begin{itemize}
    \item \textbf{Speicher:} Sämtliche Daten liegen im Hauptspeicher. Nebst den vorhandenen Daten ist nur eine feste Anzahl von (Hilfs-) Variablen erlaubt (In-place Algorithmen). 
    \item \textbf{Effizienz:} Nur Vergleiche (Comparison) und Zuweisungen (Move) werden betrachtet.
    (Bei den Zuweisungen werden nur die Zuweisungen der zu sortierenden Daten berücksichtigt. Zuweisungen von Indizes werden vernachlässigt, da sie in der Regel wesentlich schneller ablaufen, da sie sicher in Registern liegen. Auch die Vergleiche der Indizes werden vernachlässigt.)
    \item \textbf{Stabilität:} Ein Sortierverfahren ist dann stabil, wenn es die relative Reihenfolge gleicher Schlüssel beibehält. (Beispielsweise soll eine alphabetisch geordnete Liste von Wettkämpfern, die nach den Wettkampfnoten sortiert wird, die alphabetische Reihenfolge der Wettkämpfer bei gleichen Noten beibehalten.)
\end{itemize}

\subsubsection{Einfache Sortierverfahren}
\textbf{Variante 1: Direktes Aussuchen (Straight selection)}\\
Beim direkten Aussuchen wird der noch unsortierte Teil nach seinem kleinsten Element durchsucht. Dieses wird dann mit dem ersten Element im unsortierten Teil vertauscht.\\
Komplexität der Vergleiche: $O(n^2)$\\
Komplexität der Zuweisungen: $O(n)$\\
\begin{multicols}{2}
\lstinputlisting[language=C++]{code/DirektesAussuchen.cpp}
\includegraphics[width=0.5\textwidth]{images/Algorithmen/DirektesAussuchen.png}
\end{multicols}
\ \newpage
\textbf{Variante 2: Direktes Einfügen (Straight insertion)}\\
Das erste Element des unsortierten Teiles wird im sortierten Teil direkt an der richtigen Stelle entsprechend seiner Grösse eingefügt. Alle grösseren bereits sortierten Elemente müssen verschoben werden.\\
Komplexität der Vergleiche: $O(n^2)$\\
Komplexität der Zuweisungen: $O(n^2)$\\
\begin{multicols}{2}
\lstinputlisting[language=C++]{code/DirektesEinfuegen.cpp}
\includegraphics[width=0.5\textwidth]{images/Algorithmen/DirektesEinfuegen.png}
\end{multicols}
\ \newline
\textbf{Variante 3: Bubblesort}\\
Jeweils benachbarte Elemente werden vertauscht, wenn sie nicht wie gewünscht geordnet sind. Bei jedem Durchlauf steigt dabei das relativ grösste Element wie eine Blase (bubble) im Wasser auf.\\
Komplexität der Vergleiche: $O(n^2)$\\
Komplexität der Zuweisungen: $O(n^2)$\\
\begin{multicols}{2}
\lstinputlisting[language=C++]{code/Bubblesort.cpp}
\includegraphics[width=0.5\textwidth]{images/Algorithmen/Bubblesort.png}
\end{multicols}
\ \newpage

\subsubsection{Schnelle interne Sortierverfahren}
\textbf{Variante 1: Quicksort}\\
Schnell, weil das Austauschen von unsortierten Elementen über möglichst grosse Distanzen erfolgt.\\\\
Formale Beschreibung:
\begin{enumerate}
\item Wähle einen beliebigen Pivotwert aus dem Array aus.
\item Laufe von linker Arraygrenze so lange nach innen, bis ein \textbf{nicht kleineres}
Element als der Pivotwert gefunden wird. 
\item Laufe von rechter Arraygrenze so lange nach innen, bis ein \textbf{nicht grösseres} Element gefunden wird.
\item Vertausche die beiden Elemente.
\item Wiederhole Schritt 2-4 so lange, bis sich die beiden Indizes getroffen haben.
\item Das nun entstandene Teilfeld links besteht nur aus Elementen, die kleiner oder gleich dem Pivotwert sind, das Teilfeld rechts besteht nur aus Elementen, die grösser oder gleich dem Pivotwert sind. 
\item Sortiere diese beiden Teilfelder wiederum (rekursiv), bis nur noch Teilfelder mit einem einzigen Element übrig bleiben.
\end{enumerate}

--- HIER ---

Quicksort ist dann am besten, wenn beim Pivot immer der Median gefunden würde. Im schlimmsten Fall ist die Komplexität bei Quicksort $O(n^2)$. Das ist dann der Fall, wenn als Pivot immer ein Extremwert (Minimum oder Maximum) genommen würde. Dieser Fall ist für praktische Fälle nahezu unmöglich, d.h. der Worst-Case ist sehr unwahrscheinlich.\\
\begin{multicols}{2}
\lstinputlisting[language=C++]{code/DirektesEinfuegen.cpp}
\includegraphics[width=0.5\textwidth]{images/Algorithmen/DirektesEinfuegen.png}
\end{multicols}
\ \newline





1. Quicksort\\
Je grösser die Distanz beim Austauschen von Elementen ist, umso effizienter ist der Algorithmus. Dies nutzt Quicksort aus.\\
\textbf{Median}: Der Median einer Anzahl von Werten ist die Zahl, welche an der mittleren Stelle steht, wenn man die Werte nach Größe sortiert. Bsp: {4, 1, 37, 2, 1} Median = 2\\
Formale Beschreibung: \\
\begin{enumerate}
\item Wähle ein beliebiges Pivot-Element aus (Median oder mittleres Element)
\item Laufe von linker Arraygrenze so lange nach innen, bis ein Element gefunden wird, welches \textbf{grösser gleich} das Pivot ist. 
\item Laufe von rechter Arraygrenze so lange nach innen, bis ein Element gefunden wird, welches \textbf{kleiner gleich} das Pivot ist. 
\item Vertausche die beiden Elemente
\item Wiederhole Schritt 2-4 so lange, bis sich die beiden Indizes getroffen haben
\item Das linke Teilfeld enthält nur noch Werte, die $\leq$ als das Pivot sind. Das rechte Teilfeld enthält nur noch Werte, die $\geq$ als das Pivot sind. 
\item Sortiere diese Felder wiederum rekursiv, bis nur noch Teilfelder mit einem einzigen Element übrig sind.
\end{enumerate}

\begin{lstlisting}[style=C]
void quickSort(int leftBound, int rightBound)
{
	int left = leftBound; // Index der linken Grenze
	int right = rightBound; // Index der rechten Grenze
	int pivot = data[(left+right) / 2]; // Gewähltes Element in der Mitte
	do
	{
		while (data[left] < pivot)
			left++;
		while (data[right] > pivot)
			right--;
		if (left <= right)
		{
			if (left < right)
				swap(left, right);
			left++;
			right--;
		}
	} while (left <= right);
	if (leftBound < right)
		quickSort(leftBound, right);
	if (rightBound > left)
		quickSort(left, rightBound);
}
\end{lstlisting} 
Bester Fall: Wenn als Pivot jedes mal der Median des Feldes getroffen wird $\rightarrow O(nlog(n))$\\
Schlechtester Fall: Wenn jedes Mal das Minimum oder Maximum getrofffen wird $\rightarrow O(n^2))$\\

Beispiel von Quicksort: \\
\begin{center}
{\includegraphics[width=0.5\textwidth]{images/Algorithmen/Quicksort.png}}
\label{Fig: Quicksort}
\end{center}

Wahl des Pivots ist wichtig. Ein Ansatz ist die median-of-3 Strategie, bei welcher der Median aus dem ersten, letzten und mittleren Element eines Bereiches gebildet und als Pivot verwendet. Bei bösartig zusammengestellten Listen (median-of-3-killers) kann dies jedoch zu Problemen führen.\\

\subsubsection{Effizienz verschiedener interner Suchalgorithmen}
\begin{tabular}{|c|c|c|c|c|}
\hline
Algorithmus & $C_{min}$ & $C_{ave}$ & $C_{max}$ & $O_C(\cdot)$ \\
\hline
Straight Selection & $\frac{n}{2}\cdot (n-1)$ & $\frac{n}{2}\cdot (n-1)$ & $\frac{n}{2}\cdot (n-1)$ & $n^2$ \\
\hline
Straight Insertion & $n-1$ & $\frac{1}{4}\cdot (n^2 + 3n -4)$ & $\frac{1}{2}\cdot (n^2+n-2)$ & $n^2$ \\
\hline
Bubblesort & $\frac{n}{2}\cdot (n-1)$ & $\frac{n}{2}\cdot (n-1)$ & $\frac{n}{2}\cdot (n-1)$ & $n^2$ \\
\hline
Quicksort & $n\cdot ld(n)$ & $2\cdot n \cdot ln(n)$ & $\frac{1}{2}\cdot (n+3)(n+2)-10$ & $n^2$ \\
\hline
\end{tabular}

\subsubsection{Systemfunktionen für Suchen und Sortieren}
Siehe Skript 00-Algorithmen.pdf S.25
